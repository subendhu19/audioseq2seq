{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import pickle\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))\n",
    "test_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<pad>': [0, 1], '<unk>': [1, 1], '<BOS>': [2, 1], '<EOS>': [3, 1]}\n",
    "for item in train_dataset + test_dataset:\n",
    "    words = item[2].split(' ')\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word][1] += 1\n",
    "        else:\n",
    "            vocabulary[word] = [len(vocabulary), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    name, audio, words = x\n",
    "    split_words = ['<BOS>'] + words.split(' ') + ['<EOS>']\n",
    "    return audio, np.array([vocabulary[word][0] for word in split_words]), float(len(audio)), float(len(split_words))\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[1]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Processing Time={:.2f}s, #Samples={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Processing Time=3.28s, #Samples=2703\n",
      "Done! Processing Time=3.72s, #Samples=2703\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=2703, batch_num=82\n",
      "  key=[13, 22, 31, 40, 49, 58, 67, 76, 85, 94]\n",
      "  cnt=[837, 805, 531, 282, 120, 70, 30, 15, 8, 5]\n",
      "  batch_size=[46, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "learning_rate, batch_size = 0.005, 32\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "grad_clip = None\n",
    "log_interval = 5\n",
    "\n",
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "[[[ -5.684265    -5.481247   -31.677723   ... -21.189108     1.9651077\n",
      "    -1.8781087 ]\n",
      "  [ -5.8898716   -3.5039613  -30.943327   ... -28.498064    -4.1945057\n",
      "   -10.528649  ]\n",
      "  [ -6.200805    -1.0122288  -26.037184   ... -21.092169    -0.6559814\n",
      "   -12.771611  ]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[ -9.333745   -15.022931   -23.424726   ... -15.981082     1.5638912\n",
      "    -2.4987006 ]\n",
      "  [ -9.199183   -15.421494   -26.592268   ...  -7.841849     7.154292\n",
      "     1.0559975 ]\n",
      "  [ -9.145427   -15.609611   -27.623438   ...  -9.7564335   -0.18035188\n",
      "     0.16993964]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[ -8.486518   -19.346945    -2.9519491  ...  14.770905     3.164077\n",
      "    -1.0667865 ]\n",
      "  [ -8.54524    -18.859556    -1.1303926  ...  14.088975     0.9802226\n",
      "    -0.8749154 ]\n",
      "  [ -8.573369   -17.298973    -1.234618   ...  15.442868     2.5164766\n",
      "    -2.5829244 ]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-14.0967655  -17.04211      0.5386413  ...   4.407993    -8.726612\n",
      "    -8.710908  ]\n",
      "  [-13.526028   -20.914093     3.8452096  ...  10.556576     8.937594\n",
      "    -8.121687  ]\n",
      "  [-12.329824   -26.647257     9.907613   ...  16.484556     8.356734\n",
      "   -16.450275  ]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[-13.072072   -19.73856     -3.8883703  ...   1.7684336   -5.0533094\n",
      "    -9.912859  ]\n",
      "  [-12.813339   -21.800154    -2.7266734  ...  -2.399704    -1.4428432\n",
      "    -1.9522431 ]\n",
      "  [-12.755414   -21.005226    -2.9958763  ...  -5.2722335   -4.7101607\n",
      "     0.08788972]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[ -8.134973   -11.334011   -25.60325    ... -12.790935    -0.737592\n",
      "     2.6594765 ]\n",
      "  [ -7.9660707   -9.933978   -21.40218    ... -10.925345     1.1562392\n",
      "     1.9720136 ]\n",
      "  [ -7.7514515  -15.914311   -23.420994   ...  -7.9971156   -5.8227825\n",
      "    -0.83368015]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]]\n",
      "<NDArray 46x572x13 @cpu_shared(0)>, \n",
      "[[2.000e+00 2.590e+02 3.580e+02 1.343e+03 6.390e+02 2.030e+02 8.000e+01\n",
      "  2.100e+01 6.073e+03 6.074e+03 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.457e+03 8.000e+00 9.430e+02 4.043e+03 2.100e+01 4.044e+03\n",
      "  3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.100e+01 1.047e+03 5.862e+03 8.300e+01 9.000e+00 1.060e+03\n",
      "  1.530e+02 5.120e+02 7.600e+01 8.270e+02 3.000e+00 0.000e+00]\n",
      " [2.000e+00 2.300e+01 2.100e+01 1.031e+03 8.227e+03 1.055e+03 5.520e+02\n",
      "  5.520e+02 1.730e+02 1.240e+02 8.000e+00 1.105e+03 3.000e+00]\n",
      " [2.000e+00 2.100e+01 6.800e+01 3.830e+02 5.600e+01 4.827e+03 7.600e+01\n",
      "  2.100e+01 5.184e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 6.870e+02 5.500e+01 5.500e+02 8.400e+01 5.530e+02 8.390e+02\n",
      "  2.260e+02 2.949e+03 6.300e+01 1.280e+02 3.000e+00 0.000e+00]\n",
      " [2.000e+00 1.046e+03 1.047e+03 1.048e+03 2.700e+02 1.049e+03 3.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.400e+01 6.280e+02 7.600e+01 3.600e+01 2.596e+03 2.300e+01\n",
      "  2.632e+03 2.000e+01 5.000e+01 2.633e+03 2.634e+03 3.000e+00]\n",
      " [2.000e+00 7.210e+02 1.884e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.590e+02 1.500e+01 4.502e+03 8.000e+01 2.540e+02 2.800e+01\n",
      "  5.600e+01 2.900e+01 1.500e+01 1.298e+03 3.000e+00 0.000e+00]\n",
      " [2.000e+00 5.300e+01 6.692e+03 7.100e+01 7.790e+02 5.563e+03 2.752e+03\n",
      "  3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.690e+02 1.781e+03 2.100e+01 3.172e+03 3.999e+03 3.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.540e+02 4.231e+03 1.900e+01 2.300e+01 8.620e+02 5.040e+03\n",
      "  2.934e+03 2.590e+02 1.610e+02 2.260e+02 5.041e+03 3.000e+00]\n",
      " [2.000e+00 1.140e+02 7.020e+02 3.328e+03 4.900e+01 1.283e+03 8.270e+02\n",
      "  3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.400e+01 4.220e+02 2.720e+02 1.500e+01 3.700e+02 5.000e+01\n",
      "  2.610e+02 1.085e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.180e+02 2.400e+01 3.520e+02 2.100e+01 2.969e+03 8.000e+00\n",
      "  2.100e+01 2.080e+02 2.970e+03 2.958e+03 3.000e+00 0.000e+00]\n",
      " [2.000e+00 2.300e+01 2.247e+03 2.248e+03 2.153e+03 2.590e+02 1.610e+02\n",
      "  2.260e+02 8.000e+01 1.940e+02 2.249e+03 2.250e+03 3.000e+00]\n",
      " [2.000e+00 8.600e+01 2.949e+03 1.277e+03 8.000e+00 7.010e+02 3.350e+03\n",
      "  3.351e+03 1.610e+02 2.260e+02 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.280e+02 1.082e+03 2.100e+01 2.455e+03 7.530e+02 2.456e+03\n",
      "  1.330e+02 1.290e+02 2.457e+03 2.300e+01 2.458e+03 3.000e+00]\n",
      " [2.000e+00 7.260e+02 1.610e+02 3.600e+01 2.767e+03 3.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.191e+03 1.240e+03 2.320e+02 1.241e+03 1.242e+03 3.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 4.000e+01 8.000e+00 9.430e+02 1.378e+03 1.201e+03 2.590e+02\n",
      "  1.500e+01 2.160e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 8.600e+01 2.500e+01 7.730e+02 2.000e+01 5.000e+01 2.100e+01\n",
      "  3.947e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.590e+02 7.790e+02 6.870e+02 2.293e+03 3.690e+02 2.540e+02\n",
      "  1.055e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.001e+03 7.110e+02 3.000e+01 2.741e+03 2.100e+01 2.555e+03\n",
      "  5.000e+01 1.500e+01 1.545e+03 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.218e+03 6.870e+02 5.830e+02 4.523e+03 1.348e+03 6.010e+02\n",
      "  1.023e+03 3.390e+02 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 7.260e+02 7.790e+02 5.400e+01 1.055e+03 2.100e+01 1.044e+03\n",
      "  3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.100e+03 3.300e+02 1.780e+02 1.101e+03 2.300e+01 4.200e+02\n",
      "  1.110e+02 3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.157e+03 4.479e+03 3.100e+01 2.320e+02 9.580e+02 4.600e+01\n",
      "  4.480e+03 7.790e+02 1.900e+01 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 5.400e+01 5.044e+03 5.045e+03 3.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.540e+02 9.300e+01 3.650e+02 1.500e+01 1.646e+03 1.442e+03\n",
      "  5.900e+01 3.100e+01 3.798e+03 3.799e+03 3.000e+00 0.000e+00]\n",
      " [2.000e+00 2.100e+01 6.300e+01 6.366e+03 9.300e+01 6.367e+03 6.160e+02\n",
      "  7.600e+01 1.348e+03 5.940e+02 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 7.100e+01 1.610e+02 8.600e+01 3.572e+03 3.350e+02 4.900e+01\n",
      "  2.300e+01 2.456e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 5.300e+01 9.710e+02 1.000e+02 2.180e+02 1.692e+03 5.000e+01\n",
      "  2.140e+02 6.706e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.400e+01 9.300e+01 2.180e+02 6.108e+03 3.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.089e+03 8.000e+00 3.295e+03 2.711e+03 2.100e+01 2.360e+02\n",
      "  3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 6.201e+03 6.202e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 6.870e+02 3.404e+03 4.900e+01 5.000e+01 1.420e+02 2.260e+02\n",
      "  1.343e+03 2.791e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 7.867e+03 2.248e+03 7.868e+03 1.479e+03 3.602e+03 1.288e+03\n",
      "  2.300e+01 4.200e+01 1.298e+03 3.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 3.280e+02 6.218e+03 1.200e+01 4.797e+03 2.300e+01 5.250e+02\n",
      "  2.320e+02 2.599e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 2.300e+01 2.540e+02 4.231e+03 4.640e+02 2.000e+01 8.000e+00\n",
      "  2.100e+01 2.350e+02 1.780e+02 3.390e+02 9.970e+02 3.000e+00]\n",
      " [2.000e+00 2.400e+01 1.508e+03 2.590e+02 2.100e+01 1.509e+03 9.300e+01\n",
      "  2.700e+02 2.030e+02 1.348e+03 1.012e+03 3.000e+00 0.000e+00]\n",
      " [2.000e+00 2.590e+02 1.610e+02 2.260e+02 7.217e+03 1.343e+03 1.055e+03\n",
      "  2.100e+01 1.497e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.203e+03 1.442e+03 2.725e+03 8.300e+01 5.071e+03 3.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.100e+01 3.900e+01 5.074e+03 4.797e+03 5.000e+01 2.100e+01\n",
      "  2.465e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e+00 8.300e+01 9.000e+00 7.993e+03 1.159e+03 2.100e+01 6.898e+03\n",
      "  8.000e+00 2.100e+01 7.994e+03 3.900e+01 3.578e+03 3.000e+00]]\n",
      "<NDArray 46x13 @cpu_shared(0)>, \n",
      "[336. 329. 435. 551. 304. 315. 295. 346. 262. 356. 272. 233. 469. 287.\n",
      " 351. 299. 365. 312. 572. 246. 256. 361. 208. 357. 294. 267. 230. 440.\n",
      " 318. 236. 334. 379. 325. 348. 184. 324. 195. 276. 354. 324. 389. 384.\n",
      " 309. 263. 247. 426.]\n",
      "<NDArray 46 @cpu_shared(0)>, \n",
      "[11.  8. 12. 13. 10. 12.  7. 13.  4. 12.  8.  7. 13.  8. 10. 12. 13. 11.\n",
      " 13.  6.  7. 10.  9.  9. 11. 10.  8.  9. 11.  5. 12. 11. 10. 10.  6.  8.\n",
      "  4. 10. 11. 10. 13. 12. 10.  7.  9. 13.]\n",
      "<NDArray 46 @cpu_shared(0)>)\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(train_dataloader):\n",
    "    if i >= 1:\n",
    "        break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from io import open\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "class SubSampler(gluon.HybridBlock):\n",
    "    def __init__(self, size=3, prefix=None, params=None):\n",
    "        super(SubSampler, self).__init__(prefix=prefix, params=params)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        subsampled = F.Pooling(masked_encoded.swapaxes(0,2), kernel=(self.size), pool_type='max', stride=self.size).swapaxes(0,2)\n",
    "        return subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(Block):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn1 = rnn.GRU(hidden_size, input_size=self.input_size)\n",
    "            self.subsampler = SubSampler(size=3)\n",
    "            self.rnn2 = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, lengths):\n",
    "        input = input.swapaxes(0, 1)\n",
    "        output, hidden = self.rnn1(input, hidden)\n",
    "        subsampled = self.subsampler(output, lengths)\n",
    "        output, hidden = self.rnn2(subsampled, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Block):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "            self.rnn = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "            self.out = nn.Dense(output_size, in_units=self.hidden_size, flatten=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).swapaxes(0, 1)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=13, hidden_size=256)\n",
    "decoder = DecoderRNN(hidden_size=256, output_size=34798)\n",
    "\n",
    "encoder.initialize(mx.init.Xavier(), ctx=context)\n",
    "decoder.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (audio, words, alength, wlength) in enumerate(train_dataloader):\n",
    "            wc = alength.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += audio.shape[1]\n",
    "            epoch_sent_num += audio.shape[1]\n",
    "            with autograd.record():\n",
    "                encoder_hidden = encoder.initHidden(len(audio), context)\n",
    "                encoder_outputs, encoder_hidden = encoder(audio.as_in_context(context), encoder_hidden, alength)\n",
    "                # TODO: Get the last hidden state/attended state according to the lengths\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_outputs, decoder_hidden = decoder(words.as_in_context(context), decoder_hidden)\n",
    "                decoder_outputs = decoder_outputs.swapaxes(0,1)\n",
    "                L = loss(decoder_outputs, words.as_in_context(context)).sum()\n",
    "            L.backward()\n",
    "            \n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K fps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "              'test avg loss {:.6f}, throughput {:.2f}K fps'.format(\n",
    "                  epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 5/82] elapsed 135.79 s, avg loss 0.311807, throughput 0.79K fps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-d6769ad95e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-2f6a4489f39e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, context, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlog_interval_L\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mepoch_L\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/allennlp/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/allennlp/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(encoder, context, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
