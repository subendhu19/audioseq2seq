{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import pickle\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))\n",
    "test_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<pad>': [0, 1], '<unk>': [1, 1], '<BOS>': [2, 1], '<EOS>': [3, 1]}\n",
    "for item in train_dataset + test_dataset:\n",
    "    words = item[2].split(' ')\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word][1] += 1\n",
    "        else:\n",
    "            vocabulary[word] = [len(vocabulary), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    name, audio, words = x\n",
    "    split_words = ['<BOS>'] + words.split(' ') + ['<EOS>']\n",
    "    return audio, np.array([vocabulary[word][0] for word in split_words]), float(len(audio)), float(len(split_words))\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[1]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Processing Time={:.2f}s, #Samples={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, batch_size = 0.005, 32\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "grad_clip = None\n",
    "log_interval = 5\n",
    "\n",
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(train_dataloader):\n",
    "    if i >= 1:\n",
    "        break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from io import open\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "class SubSampler(gluon.HybridBlock):\n",
    "    def __init__(self, size=3, prefix=None, params=None):\n",
    "        super(SubSampler, self).__init__(prefix=prefix, params=params)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        subsampled = F.Pooling(masked_encoded.swapaxes(0,2), kernel=(self.size), pool_type='max', stride=self.size).swapaxes(0,2)\n",
    "        return subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(Block):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn1 = rnn.GRU(hidden_size, input_size=self.input_size)\n",
    "            self.subsampler = SubSampler(size=3)\n",
    "            self.rnn2 = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, lengths):\n",
    "        input = input.swapaxes(0, 1)\n",
    "        output, hidden = self.rnn1(input, hidden)\n",
    "        subsampled = self.subsampler(output, lengths)\n",
    "        output, hidden = self.rnn2(subsampled, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Block):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "            self.rnn = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "            self.out = nn.Dense(output_size, in_units=self.hidden_size, flatten=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).swapaxes(0, 1)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(Block):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        with self.name_scope():\n",
    "            self.encoder = EncoderRNN(input_size=input_size, hidden_size=hidden_size)\n",
    "            self.decoder = DecoderRNN(hidden_size=hidden_size, output_size=output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, lengths, words):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input, hidden, lengths)\n",
    "        # TODO: Get the last hidden state/attended state according to the lengths\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs, decoder_hidden = self.decoder(words, decoder_hidden)\n",
    "        decoder_outputs = decoder_outputs.swapaxes(0,1)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq2Seq(input_size=13, output_size=34798, hidden_size=16)\n",
    "net.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (audio, words, alength, wlength) in enumerate(train_dataloader):\n",
    "            wc = alength.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += audio.shape[1]\n",
    "            epoch_sent_num += audio.shape[1]\n",
    "            with autograd.record():\n",
    "                encoder_hidden = net.encoder.initHidden(len(audio), context)\n",
    "                decoder_outputs = net(audio.as_in_context(context), encoder_hidden, alength, words.as_in_context(context))\n",
    "                L = loss(decoder_outputs, words.as_in_context(context)).sum()\n",
    "            L.backward()\n",
    "            \n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K fps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "#         test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "#         print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "#               'test avg loss {:.6f}, throughput {:.2f}K fps'.format(\n",
    "#                   epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "#                   epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, context, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
