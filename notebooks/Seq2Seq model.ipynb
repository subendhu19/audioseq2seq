{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import pickle\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))\n",
    "test_dataset = pickle.load(open('../data/dev_processed.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<pad>': [0, 1], '<unk>': [1, 1], '<BOS>': [2, 1], '<EOS>': [3, 1]}\n",
    "for item in train_dataset + test_dataset:\n",
    "    words = item[2].split(' ')\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word][1] += 1\n",
    "        else:\n",
    "            vocabulary[word] = [len(vocabulary), 1]\n",
    "\n",
    "vocabulary_inv = {}\n",
    "for key in vocabulary:\n",
    "    vocabulary_inv[vocabulary[key][0]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    name, audio, words = x\n",
    "    split_words = ['<BOS>'] + words.split(' ') + ['<EOS>']\n",
    "    return audio, np.array([vocabulary[word][0] for word in split_words]), float(len(audio)), float(len(split_words))\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[1]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Processing Time={:.2f}s, #Samples={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Processing Time=2.48s, #Samples=2703\n",
      "Done! Processing Time=1.96s, #Samples=2703\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=2703, batch_num=82\n",
      "  key=[13, 22, 31, 40, 49, 58, 67, 76, 85, 94]\n",
      "  cnt=[837, 805, 531, 282, 120, 70, 30, 15, 8, 5]\n",
      "  batch_size=[46, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "learning_rate, batch_size = 0.005, 32\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "grad_clip = None\n",
    "log_interval = 5\n",
    "\n",
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Pad(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(train_dataloader):\n",
    "    if i >= 1:\n",
    "        break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from io import open\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "class SubSampler(gluon.HybridBlock):\n",
    "    def __init__(self, size=3, prefix=None, params=None):\n",
    "        super(SubSampler, self).__init__(prefix=prefix, params=params)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        subsampled = F.Pooling(masked_encoded.swapaxes(0,2), kernel=(self.size), pool_type='max', stride=self.size).swapaxes(0,2)\n",
    "        sub_valid_length = mx.nd.ceil(valid_length / self.size)\n",
    "        return subsampled, sub_valid_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(Block):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn1 = rnn.GRU(hidden_size, input_size=self.input_size)\n",
    "            self.subsampler = SubSampler(size=3)\n",
    "            self.rnn2 = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, lengths):\n",
    "        input = input.swapaxes(0, 1)\n",
    "        output, hidden1 = self.rnn1(input, hidden)\n",
    "        subsampled, sub_lengths = self.subsampler(output, lengths)\n",
    "        output, hidden2 = self.rnn2(subsampled, hidden)\n",
    "        return output, hidden2, sub_lengths\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Block):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "            self.rnn = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "            self.out = nn.Dense(output_size, in_units=self.hidden_size, flatten=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output, hidden = self.rnn(output, [hidden[0].swapaxes(0, 1)])\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batchsize, ctx):\n",
    "        return [F.zeros((1, batchsize, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(Block):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        with self.name_scope():\n",
    "            self.encoder = EncoderRNN(input_size=input_size, hidden_size=hidden_size)\n",
    "            self.decoder = DecoderRNN(hidden_size=hidden_size, output_size=output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, lengths, words):\n",
    "        encoder_outputs, encoder_hidden, encoder_out_lengths = self.encoder(input, hidden, lengths)\n",
    "        r_enc_lens = mx.ndarray.repeat(encoder_out_lengths.expand_dims(1), repeats=encoder_outputs.shape[2], axis=1)\n",
    "        encoder_final_states = mx.ndarray.pick(encoder_outputs.swapaxes(0,1), \n",
    "                                               r_enc_lens,\n",
    "                                               axis=1).expand_dims(0)\n",
    "        # CURRENT: Get the last hidden state according to the lengths\n",
    "        # TODO   : Get the attended state\n",
    "        decoder_hidden = [encoder_final_states.swapaxes(0, 1)]\n",
    "        decoder_outputs, decoder_hidden = self.decoder(words.swapaxes(0,1), decoder_hidden)\n",
    "        decoder_outputs = decoder_outputs.swapaxes(0,1)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.loss import Loss as Loss\n",
    "\n",
    "class SoftmaxSequenceCrossEntropyLoss(Loss):\n",
    "    def __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None,\n",
    "                 batch_axis=0, **kwargs):\n",
    "        super(SoftmaxSequenceCrossEntropyLoss, self).__init__(\n",
    "            weight, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._sparse_label = sparse_label\n",
    "        self._from_logits = from_logits\n",
    "    \n",
    "    def hybrid_forward(self, F, pred, label, valid_length):\n",
    "        if not self._from_logits:\n",
    "            pred = F.log_softmax(pred, self._axis)\n",
    "        loss = mx.nd.squeeze(-F.pick(pred, label, axis=self._axis, keepdims=True), axis=2)\n",
    "        loss = F.SequenceMask(loss.swapaxes(0,1), \n",
    "                              sequence_length=valid_length,\n",
    "                              use_sequence_length=True).swapaxes(0,1)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq2Seq(input_size=13, output_size=34798, hidden_size=16)\n",
    "net.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "class beamDecoder(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "    def __call__(self, inputs, states):\n",
    "        outputs, states = self._model.decoder(mx.nd.expand_dims(inputs, axis=0), states)\n",
    "        return outputs[0], [states[0].swapaxes(0,1)]\n",
    "\n",
    "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)\n",
    "eos_id = vocabulary['<EOS>'][0]\n",
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,\n",
    "                                           decoder=beamDecoder(net),\n",
    "                                           eos_id=eos_id,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_accuracy(s1, l1, s2, l2):\n",
    "    s1 = mx.nd.cast(s1, dtype='int32')\n",
    "    l1 = mx.nd.cast(l1, dtype='int32')\n",
    "    s2 = mx.nd.cast(s2, dtype='int32')\n",
    "    l2 = mx.nd.cast(l2, dtype='int32')\n",
    "    padding = mx.nd.zeros((s1.shape[0], abs(s1.shape[1] - s2.shape[1])), dtype=s2.dtype)\n",
    "    if s1.shape[1] > s2.shape[1]:\n",
    "        s2 = mx.nd.concat(s2, padding, dim=1)\n",
    "    else:\n",
    "        s1 = mx.nd.concat(s1, padding, dim=1) \n",
    "    accs = F.SequenceMask((s1 == s2).swapaxes(0,1), \n",
    "                          sequence_length=mx.nd.minimum(l1, l2), \n",
    "                          use_sequence_length=True)\n",
    "    return (mx.nd.cast(accs.sum(), dtype='float32')/mx.nd.cast(l1.sum(), dtype='float32')).asnumpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, context):\n",
    "    for i, (audio, words, alength, wlength) in enumerate(test_dataloader):\n",
    "        encoder_hidden = net.encoder.initHidden(len(audio), context)\n",
    "        encoder_outputs, encoder_hidden, encoder_out_lengths = net.encoder(audio.as_in_context(context), encoder_hidden, alength)\n",
    "        r_enc_lens = mx.ndarray.repeat(encoder_out_lengths.expand_dims(1), repeats=encoder_outputs.shape[2], axis=1)\n",
    "        encoder_final_states = mx.ndarray.pick(encoder_outputs.swapaxes(0,1), \n",
    "                                               r_enc_lens,\n",
    "                                               axis=1).expand_dims(0)\n",
    "        decoder_hidden = [encoder_final_states.swapaxes(0, 1)]\n",
    "\n",
    "        inputs = mx.nd.array([2] * words.shape[0])\n",
    "        samples, scores, valid_lengths = beam_sampler(inputs, decoder_hidden)\n",
    "        best_samples = samples[:,0,1:-1]\n",
    "        best_vlens = valid_lengths[:,0]\n",
    "        return get_sequence_accuracy(words, wlength, best_samples, best_vlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = SoftmaxSequenceCrossEntropyLoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (audio, words, alength, wlength) in enumerate(train_dataloader):\n",
    "            wc = alength.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += audio.shape[1]\n",
    "            epoch_sent_num += audio.shape[1]\n",
    "            with autograd.record():\n",
    "                encoder_hidden = net.encoder.initHidden(len(audio), context)\n",
    "                decoder_outputs = net(audio.as_in_context(context), encoder_hidden, alength, words.as_in_context(context))\n",
    "                L = loss(decoder_outputs, words.as_in_context(context), wlength).sum()\n",
    "            L.backward()\n",
    "            \n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K fps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        \n",
    "        end_epoch_time = time.time()\n",
    "        test_acc = evaluate(net, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "              'throughput {:.2f}K fps'.format(\n",
    "              epoch, epoch_L / epoch_sent_num, test_acc,\n",
    "              epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, context, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(sampler, inputs, begin_states, num_print_outcomes):\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    print(valid_lengths.shape)\n",
    "    print('Generation Result:')\n",
    "    \n",
    "    for sample_id in range(samples.shape[0]):\n",
    "        sample = samples[sample_id].asnumpy()\n",
    "        score = scores[sample_id].asnumpy()\n",
    "        valid_length = valid_lengths[sample_id].asnumpy()\n",
    "\n",
    "        for i in range(num_print_outcomes):\n",
    "            sentence = []\n",
    "            for ele in sample[i][:valid_length[i]]:\n",
    "                sentence.append(vocabulary_inv[ele])\n",
    "            print([' '.join(sentence), score[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mx.nd.array([2] * 32)\n",
    "begin_states = [mx.nd.array([[[0]*16]] * 32)]\n",
    "generate_sequences(beam_sampler, inputs, begin_states, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285969734192"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mx.nd.array([[2,2,2,2], [3,3,3,3]])\n",
    "al = mx.nd.array([3,4])\n",
    "b = mx.nd.array([[2,2,1], [2,3,3]])\n",
    "bl = mx.nd.array([2,3])\n",
    "get_sequence_accuracy(a,al,b,bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
